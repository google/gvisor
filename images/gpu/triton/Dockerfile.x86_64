FROM google/cloud-sdk:541.0.0-slim AS downloader
RUN gcloud config set auth/disable_credentials true
RUN gsutil -m cp -r gs://gvisor/tests/models/llama-2-7b-chat-hf /
RUN mkdir -p /engines
RUN gsutil -m cp -r gs://gvisor/tests/l4/engines/llama-2-7b-chat-hf /engines/

FROM nvcr.io/nvidia/tritonserver:25.08-trtllm-python-py3
# --- Build Arguments ---
# Use ARG for values you want to configure at build time.
# Default values are provided for convenience.
ARG TOKENIZER_DIR=/llama-2-7b-chat-hf
ARG ENGINE_DIR=/engines/llama-2-7b-chat-hf/fp8/1-gpu
ARG MAX_BATCH_SIZE=1
ARG INSTANCE_COUNT=1
ARG TOKENIZER_TYPE=auto
ARG DECOUPLED_MODE=true
ARG MODEL_FOLDER=/models/
ARG MAX_QUEUE_DELAY_MS=10000
ARG TRITON_BACKEND=tensorrtllm
ARG LOGITS_DATATYPE="TYPE_FP32"
ARG FILL_TEMPLATE_SCRIPT=/TensorRT-LLM/triton_backend/tools/fill_template.py

COPY --from=downloader ${TOKENIZER_DIR} ${TOKENIZER_DIR}
COPY --from=downloader /engines /engines
RUN git clone https://github.com/NVIDIA/TensorRT-LLM.git /TensorRT-LLM
# --- Model Configuration ---
# Copy the model templates and configure them.
RUN cp -r /TensorRT-LLM/triton_backend/all_models/inflight_batcher_llm ${MODEL_FOLDER}
RUN cp -r ${ENGINE_DIR}/* ${MODEL_FOLDER}/tensorrt_llm/1/

# Run the template-filling commands to generate the final config.pbtxt files.
# This command uses the ARG values defined above.
RUN python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/preprocessing/config.pbtxt \
        tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${MAX_BATCH_SIZE},preprocessing_instance_count:${INSTANCE_COUNT} && \
    python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/postprocessing/config.pbtxt \
        tokenizer_dir:${TOKENIZER_DIR},triton_max_batch_size:${MAX_BATCH_SIZE},postprocessing_instance_count:${INSTANCE_COUNT} && \
    python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm_bls/config.pbtxt \
        prompt_embedding_table_data_type:TYPE_FP16,triton_max_batch_size:${MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},bls_instance_count:${INSTANCE_COUNT},logits_datatype:${LOGITS_DATATYPE} && \
    python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/ensemble/config.pbtxt \
        triton_max_batch_size:${MAX_BATCH_SIZE},logits_datatype:${LOGITS_DATATYPE} && \
    python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm/config.pbtxt \
        prompt_embedding_table_data_type:TYPE_FP16,triton_backend:${TRITON_BACKEND},triton_max_batch_size:${MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},engine_dir:${ENGINE_DIR},max_queue_delay_microseconds:${MAX_QUEUE_DELAY_MS},batching_strategy:inflight_fused_batching,encoder_input_features_data_type:TYPE_FP16,logits_datatype:${LOGITS_DATATYPE}

CMD ["tritonserver", "--model-repository=/models/"]
